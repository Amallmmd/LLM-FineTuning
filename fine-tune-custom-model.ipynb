{"cells":[{"cell_type":"code","execution_count":84,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-01T19:13:38.031605Z","iopub.status.busy":"2024-05-01T19:13:38.031189Z","iopub.status.idle":"2024-05-01T19:13:38.038137Z","shell.execute_reply":"2024-05-01T19:13:38.036733Z","shell.execute_reply.started":"2024-05-01T19:13:38.031570Z"},"trusted":true},"outputs":[],"source":["# !pip install git+https://github.com/huggingface/transformers.git  -U \n","# !pip install bitsandbytes accelerate\n","# !pip install git+https://github.com/huggingface/peft.git  -U \n","# !pip install -q -U datasets\n","# !pip install trl\n","# !pip install -U einops\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:29:19.154817Z","iopub.status.busy":"2024-05-01T17:29:19.154096Z","iopub.status.idle":"2024-05-01T17:29:19.163182Z","shell.execute_reply":"2024-05-01T17:29:19.162019Z","shell.execute_reply.started":"2024-05-01T17:29:19.154777Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:51:50.862487Z","iopub.status.busy":"2024-05-01T16:51:50.861814Z","iopub.status.idle":"2024-05-01T16:53:35.332356Z","shell.execute_reply":"2024-05-01T16:53:35.331201Z","shell.execute_reply.started":"2024-05-01T16:51:50.862456Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-01 16:51:57.537948: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-01 16:51:57.538067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-01 16:51:57.632834: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"]},{"name":"stdout","output_type":"stream","text":["Enter your token (input will not be visible):  ·····································\n","Add token as git credential? (Y/n)  n\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    GPT2Tokenizer,\n","    GPT2LMHeadModel,\n","\n","    TrainingArguments,\n","    Trainer,\n","    GenerationConfig\n",")\n","from tqdm import tqdm\n","from trl import SFTTrainer\n","import torch\n","import time\n","import pandas as pd\n","import numpy as np\n","from datasets import load_dataset\n","\n","from huggingface_hub import interpreter_login\n","\n","interpreter_login()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:53:35.334220Z","iopub.status.busy":"2024-05-01T16:53:35.333593Z","iopub.status.idle":"2024-05-01T16:53:35.342387Z","shell.execute_reply":"2024-05-01T16:53:35.341453Z","shell.execute_reply.started":"2024-05-01T16:53:35.334190Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Check GPU compatibility with bfloat16\n","\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","else:\n","  try:\n","    device = torch.device('mps')\n","  except Exception as e:\n","    device = torch.device('cpu')\n","device"]},{"cell_type":"markdown","metadata":{},"source":["# Fine Tuning Intitialization\n","* Setting up bits and bytes configuration\n","* loading Pretrained Model and Hugging face dataset\n","* Model Configuration"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:53:35.344008Z","iopub.status.busy":"2024-05-01T16:53:35.343650Z","iopub.status.idle":"2024-05-01T16:54:11.410002Z","shell.execute_reply":"2024-05-01T16:54:11.409003Z","shell.execute_reply.started":"2024-05-01T16:53:35.343956Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c5536fd3e684f7196416093ea26eff5","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ddcf2ede8da4d20b8e9fc6d59830010","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a0083174bf84f1488eeeb8136dba653","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/441k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"450a81fb45b8453cbb334733115090ed","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/447k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8710f6fa8d0543399f31e555f2e4a4cf","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f86580a26b340daad7abf186235fe41","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3637100a0f2547558426eab17daf2d2d","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f1498559d004ed6ba38816e15aa62f2","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d52e5325549f4b379de8dd42025beef3","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3025d16720c94d26889902ea3eaf0f83","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b28d68a788441ca9762ef69ee0693cf","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8ca98fa74a54eebb59130d12f260d8e","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6228f2fd66fb40d2ba125482fac867c8","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb94d17ff8844a2587bb294412387d12","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","        load_in_4bit = use_4bit,\n","        bnb_4bit_quant_type = bnb_4bit_quant_type,\n","        bnb_4bit_compute_dtype = compute_dtype,\n","        bnb_4bit_use_double_quant = False,\n","    )\n","\n","model_name='microsoft/phi-2'\n","dataset = load_dataset('neil-code/dialogsum-test')\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","#load dataset\n","device_map = {\"\": 0}\n","\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:11.411686Z","iopub.status.busy":"2024-05-01T16:54:11.411349Z","iopub.status.idle":"2024-05-01T16:54:11.421183Z","shell.execute_reply":"2024-05-01T16:54:11.420146Z","shell.execute_reply.started":"2024-05-01T16:54:11.411657Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["## configuring tokenizer "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:11.422735Z","iopub.status.busy":"2024-05-01T16:54:11.422445Z","iopub.status.idle":"2024-05-01T16:54:22.544438Z","shell.execute_reply":"2024-05-01T16:54:22.543370Z","shell.execute_reply.started":"2024-05-01T16:54:11.422708Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f29eaedfd1e74096bc8a2a46ea21e682","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d38451d9fca495d9649f3f0b2511e2b","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bbccb23ea1ae4fbcbb3b37f63dfc40c9","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74e7d7cc99214867adcda9ea9664d024","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6548041e183470b9b431b10258de208","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d3e25d276db4becb9f6f868f855076e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","# tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:22.546155Z","iopub.status.busy":"2024-05-01T16:54:22.545732Z","iopub.status.idle":"2024-05-01T16:54:22.558795Z","shell.execute_reply":"2024-05-01T16:54:22.557783Z","shell.execute_reply.started":"2024-05-01T16:54:22.546115Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'id': 'train_0',\n"," 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n"," 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n"," 'topic': 'get a check-up'}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]"]},{"cell_type":"markdown","metadata":{},"source":["## Testing Model with zero shot Inferencing"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:22.563736Z","iopub.status.busy":"2024-05-01T16:54:22.563414Z","iopub.status.idle":"2024-05-01T16:54:28.889559Z","shell.execute_reply":"2024-05-01T16:54:28.888552Z","shell.execute_reply.started":"2024-05-01T16:54:22.563706Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: Happy Birthday, this is for you, Brian.\n","#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n","#Person1#: Brian, may I have a pleasure to have a dance with you?\n","#Person2#: Ok.\n","#Person1#: This is really wonderful party.\n","#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n","#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n","#Person2#: You look great, you are absolutely glowing.\n","#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1 and Person2 are at a party, and Person1 wishes Person2 a happy birthday. Person2 thanks them for remembering and invites them to join the party. Person1 compliments Person2 and asks if they can have a dance. Person2 agrees and Person1 expresses their happiness with the party. Person2 compliments Person1's appearance and suggests they have a drink together to celebrate.\n","\n","CPU times: user 5.71 s, sys: 191 ms, total: 5.9 s\n","Wall time: 6.15 s\n"]}],"source":["%%time\n","from transformers import set_seed,pipeline\n","seed = 42\n","set_seed(seed)\n","\n","index = 10\n","\n","prompt = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n","text_generator = pipeline('text-generation', model=model,tokenizer=tokenizer)\n","\n","res = text_generator(formatted_prompt, max_length=1000)\n","# print(res[0])\n","output = res[0]['generated_text'].split('Output:\\n')[1]\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{formatted_prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-processing existing dataset\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:28.900628Z","iopub.status.busy":"2024-05-01T16:54:28.899867Z","iopub.status.idle":"2024-05-01T16:54:29.050398Z","shell.execute_reply":"2024-05-01T16:54:29.049364Z","shell.execute_reply.started":"2024-05-01T16:54:28.900590Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1999\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 499\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 499\n","    })\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]\n","dataset"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:29.052424Z","iopub.status.busy":"2024-05-01T16:54:29.052043Z","iopub.status.idle":"2024-05-01T16:54:29.067284Z","shell.execute_reply":"2024-05-01T16:54:29.065966Z","shell.execute_reply.started":"2024-05-01T16:54:29.052393Z"},"trusted":true},"outputs":[],"source":[" # function can be used to convert our input into prompt format.\n","def create_prompt_formats(sample):\n","    \"\"\"\n","    Format various fields of the sample ('instruction','output')\n","    Then concatenate them using two newline characters \n","    :param sample: Sample dictionnary\n","    \"\"\"\n","    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n","    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n","    RESPONSE_KEY = \"### Output:\"\n","    END_KEY = \"### End\"\n","    \n","    blurb = f\"\\n{INTRO_BLURB}\"\n","    instruction = f\"{INSTRUCTION_KEY}\"\n","    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n","    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n","    end = f\"{END_KEY}\"\n","    \n","    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n","\n","    formatted_prompt = \"\\n\\n\".join(parts)\n","    sample[\"text\"] = formatted_prompt\n","\n","    return sample\n","\n","# processing the prompted ones into tokenized ones.\n","from functools import partial\n","def get_max_length(model):\n","    conf = model.config\n","    max_length = None\n","    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n","        max_length = getattr(model.config, length_setting, None)\n","        if max_length:\n","            print(f\"Found max lenth: {max_length}\")\n","            break\n","    if not max_length:\n","        max_length = 1024\n","        print(f\"Using default max length: {max_length}\")\n","    return max_length\n","def preprocess_batch(batch, tokenizer, max_length):\n","    \"\"\"\n","    Tokenizing a batch\n","    \"\"\"\n","    return tokenizer(\n","        batch[\"text\"],\n","        max_length=max_length,\n","        truncation=True,\n","    )\n","  \n","def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n","    \"\"\"Format & tokenize it so it is ready for training\n","    :param tokenizer (AutoTokenizer): Model Tokenizer\n","    :param max_length (int): Maximum number of tokens to emit from tokenizer\n","    \"\"\"\n","    \n","    # Add prompt to each sample\n","    print(\"Preprocessing dataset...\")\n","    dataset = dataset.map(create_prompt_formats)#, batched=True)\n","    \n","    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n","    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n","    dataset = dataset.map(\n","        _preprocessing_function,\n","        batched=True,\n","        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n","    )\n","\n","    # Filter out samples that have input_ids exceeding max_length\n","    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n","    \n","    # Shuffle dataset\n","    dataset = dataset.shuffle(seed=seed)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:29.069540Z","iopub.status.busy":"2024-05-01T16:54:29.068794Z","iopub.status.idle":"2024-05-01T16:54:31.998907Z","shell.execute_reply":"2024-05-01T16:54:31.997867Z","shell.execute_reply.started":"2024-05-01T16:54:29.069509Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found max lenth: 2048\n","2048\n","Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38dbe2bf753a4fdcb8956d2b224c8a52","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f314ad3305d40c49163bf37bb009c64","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e46d26bef2604070afced2811161c735","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 1999\n","})\n","Preprocessing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c178789b9b814fdfa63e0f47c6851e14","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cc909143fec421589f109c667c7425a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f1eea701e2c4d809f014f76ed010ffd","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'input_ids', 'attention_mask'],\n","    num_rows: 499\n","})\n"]}],"source":["max_length = get_max_length(model)\n","print(max_length)\n","train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n","print(train_dataset)\n","eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n","print(eval_dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.001029Z","iopub.status.busy":"2024-05-01T16:54:32.000332Z","iopub.status.idle":"2024-05-01T16:54:32.007760Z","shell.execute_reply":"2024-05-01T16:54:32.006613Z","shell.execute_reply.started":"2024-05-01T16:54:32.000965Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","\n","# LoRA attention dimension\n","lora_r = 32\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 32\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","peft_config = LoraConfig(\n","    r = lora_r,\n","    lora_alpha = lora_alpha,\n","    lora_dropout= lora_dropout,\n","    bias = 'none',\n","    task_type='CAUSAL_LM',\n","    target_modules=['k_proj','v_proj','up_proj','q_proj','o_proj','gate_proj','down_proj']\n",")\n","#the model is prepared for QLoRA training\n","# original_model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.009842Z","iopub.status.busy":"2024-05-01T16:54:32.009457Z","iopub.status.idle":"2024-05-01T16:54:32.061904Z","shell.execute_reply":"2024-05-01T16:54:32.060965Z","shell.execute_reply.started":"2024-05-01T16:54:32.009805Z"},"trusted":true},"outputs":[],"source":["model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.063831Z","iopub.status.busy":"2024-05-01T16:54:32.063329Z","iopub.status.idle":"2024-05-01T16:54:32.074243Z","shell.execute_reply":"2024-05-01T16:54:32.073206Z","shell.execute_reply.started":"2024-05-01T16:54:32.063792Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.075828Z","iopub.status.busy":"2024-05-01T16:54:32.075489Z","iopub.status.idle":"2024-05-01T16:54:32.519962Z","shell.execute_reply":"2024-05-01T16:54:32.518728Z","shell.execute_reply.started":"2024-05-01T16:54:32.075799Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")\n","\n","PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): PhiForCausalLM(\n","      (model): PhiModel(\n","        (embed_tokens): Embedding(51200, 2560)\n","        (embed_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-31): 32 x PhiDecoderLayer(\n","            (self_attn): PhiSdpaAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n","              (rotary_emb): PhiRotaryEmbedding()\n","            )\n","            (mlp): PhiMLP(\n","              (activation_fn): NewGELUActivation()\n","              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n","              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n","            )\n","            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["print(model)\n","print()\n","peft_model = get_peft_model(model, peft_config)\n","print(peft_model)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.521653Z","iopub.status.busy":"2024-05-01T16:54:32.521324Z","iopub.status.idle":"2024-05-01T16:54:32.533005Z","shell.execute_reply":"2024-05-01T16:54:32.531862Z","shell.execute_reply.started":"2024-05-01T16:54:32.521606Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 15,728,640 || all params: 2,795,412,480 || trainable%: 0.5627\n"]}],"source":["peft_model.print_trainable_parameters()\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.534765Z","iopub.status.busy":"2024-05-01T16:54:32.534461Z","iopub.status.idle":"2024-05-01T16:54:32.543532Z","shell.execute_reply":"2024-05-01T16:54:32.542592Z","shell.execute_reply.started":"2024-05-01T16:54:32.534738Z"},"trusted":true},"outputs":[],"source":["\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","# num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# peft_config = LoraConfig(\n","#     r = lora_r,\n","#     lora_alpha = lora_alpha,\n","#     lora_dropout= lora_dropout,\n","#     bias = 'none',\n","#     task_type='CAUSAL_LM',\n","# )\n","\n","# Batch size per GPU for training\n","# per_device_train_batch_size = 4\n","\n","# # Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# # Number of update steps to accumulate the gradients for\n","# gradient_accumulation_steps = 1\n","# gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","# max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","# learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","# weight_decay = 0.001\n","\n","# Optimizer to use\n","# optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","# lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","# max_steps = -1\n","\n","# # Ratio of steps for a linear warmup (from 0 to learning rate)\n","# warmup_ratio = 0.03\n","# group_by_length = True\n","# logging_steps = 25"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:15:06.760926Z","iopub.status.busy":"2024-05-01T19:15:06.760482Z","iopub.status.idle":"2024-05-01T19:15:06.768842Z","shell.execute_reply":"2024-05-01T19:15:06.767574Z","shell.execute_reply.started":"2024-05-01T19:15:06.760890Z"},"trusted":true},"outputs":[],"source":["output_dir = \"./results\"\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# # Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:32.554441Z","iopub.status.busy":"2024-05-01T16:54:32.554148Z","iopub.status.idle":"2024-05-01T16:54:33.248738Z","shell.execute_reply":"2024-05-01T16:54:33.247528Z","shell.execute_reply.started":"2024-05-01T16:54:32.554415Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["import transformers\n","peft_training_args = TrainingArguments(\n","\n","    output_dir = output_dir,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    max_steps = 100,\n","    fp16=fp16,\n","    bf16=bf16,\n","    learning_rate=2e-4,\n","    optim= \"paged_adamw_8bit\",\n","    logging_steps = 25,\n","    logging_dir = \"./logs\",\n","    save_strategy=\"steps\",\n","    save_steps=25,\n","    eval_steps=25,\n","    do_eval=True,\n","    gradient_checkpointing=True,\n","    weight_decay = 0.001,\n","    overwrite_output_dir = 'True',\n","    group_by_length=True,\n","    lr_scheduler_type = \"cosine\"\n","    \n",")\n","peft_model.config.use_cache = False\n","peft_trainer = transformers.Trainer(\n","    \n","    model=peft_model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    args=peft_training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n","\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T16:54:33.250539Z","iopub.status.busy":"2024-05-01T16:54:33.250218Z","iopub.status.idle":"2024-05-01T16:54:33.257343Z","shell.execute_reply":"2024-05-01T16:54:33.256284Z","shell.execute_reply.started":"2024-05-01T16:54:33.250507Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<transformers.trainer.Trainer at 0x7c2296020df0>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["peft_trainer"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:02:18.773125Z","iopub.status.busy":"2024-05-01T17:02:18.772348Z","iopub.status.idle":"2024-05-01T17:23:05.600607Z","shell.execute_reply":"2024-05-01T17:23:05.599527Z","shell.execute_reply.started":"2024-05-01T17:02:18.773086Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 20:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.384200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.342000</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.320900</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.309800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","wandb: Network error (ReadTimeout), entering retry loop.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=100, training_loss=1.3392064666748047, metrics={'train_runtime': 1245.1753, 'train_samples_per_second': 1.285, 'train_steps_per_second': 0.08, 'total_flos': 8261267239526400.0, 'train_loss': 1.3392064666748047, 'epoch': 0.8})"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["peft_trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Save the model\n","* Require deploying into HuggingFace"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:54:33.707530Z","iopub.status.busy":"2024-05-01T17:54:33.707125Z","iopub.status.idle":"2024-05-01T17:54:34.104023Z","shell.execute_reply":"2024-05-01T17:54:34.102850Z","shell.execute_reply.started":"2024-05-01T17:54:33.707496Z"},"trusted":true},"outputs":[],"source":["peft_trainer.save_model(output_dir)\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T18:08:49.825948Z","iopub.status.busy":"2024-05-01T18:08:49.825546Z","iopub.status.idle":"2024-05-01T18:08:49.834678Z","shell.execute_reply":"2024-05-01T18:08:49.833286Z","shell.execute_reply.started":"2024-05-01T18:08:49.825914Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files in the directory: ['training_args.bin', 'checkpoint-50', 'checkpoint-25', 'checkpoint-75', 'README.md', 'adapter_config.json', 'adapter_model.safetensors', 'checkpoint-100']\n"]}],"source":["import os\n","\n","\n","# List all files in the directory\n","files = os.listdir(output_dir)\n","\n","# Print the list of files\n","print(\"Files in the directory:\", files)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:23:19.291809Z","iopub.status.busy":"2024-05-01T17:23:19.291021Z","iopub.status.idle":"2024-05-01T17:23:23.428165Z","shell.execute_reply":"2024-05-01T17:23:23.426445Z","shell.execute_reply.started":"2024-05-01T17:23:19.291760Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab3fc5d143d64d0eb40dca29e58f8896","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["base_model_id = \"microsoft/phi-2\"\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/phi-2\",\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","base_model\n","eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n","eval_tokenizer.pad_token = eval_tokenizer.eos_token\n","# tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:50:00.098696Z","iopub.status.busy":"2024-05-01T17:50:00.097961Z","iopub.status.idle":"2024-05-01T17:50:00.106121Z","shell.execute_reply":"2024-05-01T17:50:00.104854Z","shell.execute_reply.started":"2024-05-01T17:50:00.098660Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'peft.peft_model.PeftModelForCausalLM'>\n"]}],"source":["print(type(peft_model))"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:15:38.387379Z","iopub.status.busy":"2024-05-01T19:15:38.386304Z","iopub.status.idle":"2024-05-01T19:15:38.393355Z","shell.execute_reply":"2024-05-01T19:15:38.391812Z","shell.execute_reply.started":"2024-05-01T19:15:38.387340Z"},"trusted":true},"outputs":[],"source":["# new_model = 'phi-2-finetuned-summarizer-model'\n","# peft_trainer.model.save_pretrained(\"phi-2-finetuned-summarizer-model\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate the Model Qualitatively (with Human Evaluation)\n"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T18:19:18.511365Z","iopub.status.busy":"2024-05-01T18:19:18.510166Z","iopub.status.idle":"2024-05-01T18:20:11.677550Z","shell.execute_reply":"2024-05-01T18:20:11.676217Z","shell.execute_reply.started":"2024-05-01T18:19:18.511324Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]},{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","Instruct: Summarize the following conversation.\n","#Person1#: You're finally here! What took so long?\n","#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n","#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n","#Person2#: I don't think it can be avoided, to be honest.\n","#Person1#: perhaps it would be better if you started taking public transport system to work.\n","#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n","#Person1#: It would be better for the environment, too.\n","#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n","#Person1#: Taking the subway would be a lot less stressful than driving as well.\n","#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n","#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n","#Person2#: That's true. I could certainly use the exercise!\n","#Person1#: So, are you going to quit driving to work then?\n","#Person2#: Yes, it's not good for me or for the environment.\n","Output:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","#Person2# got stuck in traffic and #Person1# suggests that #Person2# should consider taking public transport system to work.\n","\n","\n","CPU times: user 49 s, sys: 4.22 s, total: 53.2 s\n","Wall time: 53.2 s\n"]}],"source":["%%time\n","from transformers import set_seed,pipeline\n","seed = 42\n","set_seed(seed)\n","\n","index = 5\n","\n","dialogue = dataset['test'][index]['dialogue']\n","test_summary = dataset['test'][index]['summary']\n","\n","testing_prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","text_generator_peft = pipeline('text-generation', model=peft_model,tokenizer=eval_tokenizer)\n","\n","res_peft = text_generator_peft(testing_prompt, max_length=500)\n","# print(res[0])\n","peft_output = res_peft[0]['generated_text'].split('Output:\\n')[1]\n","prefix, success, result = peft_output.partition('###')\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{testing_prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{test_summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION :\\n{prefix}')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate the Model Qualitatively (with ROUGE Metric)\n"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T18:23:25.182102Z","iopub.status.busy":"2024-05-01T18:23:25.181303Z","iopub.status.idle":"2024-05-01T18:23:54.057583Z","shell.execute_reply":"2024-05-01T18:23:54.056409Z","shell.execute_reply.started":"2024-05-01T18:23:25.182065Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36f8de063e3e4b93adfae70daed5abd6","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["omodel = AutoModelForCausalLM.from_pretrained(base_model_id, \n","                                                      device_map='auto',\n","                                                      quantization_config=bnb_config,\n","                                                      trust_remote_code=True,\n","                                                      use_auth_token=True)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T18:50:30.746917Z","iopub.status.busy":"2024-05-01T18:50:30.746498Z","iopub.status.idle":"2024-05-01T18:56:51.885206Z","shell.execute_reply":"2024-05-01T18:56:51.883915Z","shell.execute_reply.started":"2024-05-01T18:50:30.746879Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Human Baseline Summaries</th>\n","      <th>Peft Model Summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>#Person2# gets stuck in traffic and #Person1# ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>#Person1# suggests that #Person2# should consi...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>#Person2# got stuck in traffic again and #Pers...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>#Person1# surprises #Person2# with a birthday ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            Human Baseline Summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                                Peft Model Summaries  \n","0  #Person1# asks Ms. Dawson to take a dictation ...  \n","1  #Person1# asks Ms. Dawson to take a dictation ...  \n","2  #Person1# asks Ms. Dawson to take a dictation ...  \n","3  #Person2# gets stuck in traffic and #Person1# ...  \n","4  #Person1# suggests that #Person2# should consi...  \n","5  #Person2# got stuck in traffic again and #Pers...  \n","6  Kate tells #Person1# that Masha and Hero are g...  \n","7  #Person1# tells Kate that Masha and Hero are g...  \n","8  Kate tells #Person1# that Masha and Hero are g...  \n","9  #Person1# surprises #Person2# with a birthday ...  "]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","peft_model_summaries = []\n","\n","for idx, dialogue in enumerate(dialogues):\n","    human_baseline_text_output = human_baseline_summaries[idx]\n","    testing_prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n","    \n","    # Initialize the text generation pipeline with the peft_model\n","    text_generator = pipeline('text-generation', model=peft_model, tokenizer=eval_tokenizer)\n","    \n","    # Generate summary using the peft_model\n","    peft_model_res = text_generator(testing_prompt, max_length=500)\n","    peft_model_output = peft_model_res[0]['generated_text']  # Correctly access the generated text\n","    peft_model_text_output = peft_model_output.split('Output:\\n')[1]\n","    \n","    # Assuming you want to keep the human baseline summaries for comparison\n","    original_model_summaries.append(human_baseline_text_output)\n","    peft_model_summaries.append(peft_model_text_output)\n","\n","# Create a DataFrame to compare the summaries\n","zipped_summaries = list(zip(human_baseline_summaries, peft_model_summaries))\n","df = pd.DataFrame(zipped_summaries, columns=['Human Baseline Summaries', 'Peft Model Summaries'])\n","df\n"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:13:06.441018Z","iopub.status.busy":"2024-05-01T19:13:06.439926Z","iopub.status.idle":"2024-05-01T19:13:06.450875Z","shell.execute_reply":"2024-05-01T19:13:06.449740Z","shell.execute_reply.started":"2024-05-01T19:13:06.440957Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n","Instruct: Summarize the following conversation.\n","#Person1#: You're finally here! What took so long?\n","#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n","#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n","#Person2#: I don't think it can be avoided, to be honest.\n","#Person1#: perhaps it would be better if you started taking public transport system to work.\n","#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n","#Person1#: It would be better for the environment, too.\n","#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n","#Person1#: Taking the subway would be a lot less stressful than driving as well.\n","#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n","#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n","#Person2#: That's true. I could certainly use the exercise!\n","#Person1#: So, are you going to quit driving to work then?\n","#Person2#: Yes, it's not good for me or for the environment.\n","Output:\n","#Person2# got stuck in traffic again and #Person1# suggests #Person2# should consider taking public transport system to work.\n","\n","### End of Summarization ###\n","\n"]}],"source":["print(df['Human Baseline Summaries'][5])\n","print(df['Peft Model Summaries'][5])"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:09:25.058508Z","iopub.status.busy":"2024-05-01T19:09:25.058109Z","iopub.status.idle":"2024-05-01T19:09:25.064502Z","shell.execute_reply":"2024-05-01T19:09:25.063205Z","shell.execute_reply.started":"2024-05-01T19:09:25.058476Z"},"trusted":true},"outputs":[],"source":["# !pip install evaluate\n","# !pip install rouge_score"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T18:59:48.954527Z","iopub.status.busy":"2024-05-01T18:59:48.953596Z","iopub.status.idle":"2024-05-01T18:59:50.064276Z","shell.execute_reply":"2024-05-01T18:59:50.063142Z","shell.execute_reply.started":"2024-05-01T18:59:48.954468Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ORIGINAL MODEL:\n","{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n","PEFT MODEL:\n","{'rouge1': 0.30900079080332343, 'rouge2': 0.09655350199168583, 'rougeL': 0.22419319139508476, 'rougeLsum': 0.22843246961901034}\n","Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n","rouge1: -69.10%\n","rouge2: -90.34%\n","rougeL: -77.58%\n","rougeLsum: -77.16%\n"]}],"source":["import evaluate\n","\n","rouge = evaluate.load('rouge')\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_summaries,\n","    references=human_baseline_summaries[0:len(peft_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('PEFT MODEL:')\n","print(peft_model_results)\n","\n","print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","\n","improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(peft_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')\n"]},{"cell_type":"markdown","metadata":{},"source":["#\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:02:07.729811Z","iopub.status.idle":"2024-05-01T17:02:07.730498Z","shell.execute_reply":"2024-05-01T17:02:07.730278Z","shell.execute_reply.started":"2024-05-01T17:02:07.730255Z"},"trusted":true},"outputs":[],"source":["# base_model_id = \"microsoft/phi-2\"\n","# base_model = AutoModelForCausalLM.from_pretrained(\n","#     \"microsoft/phi-2\",\n","#     quantization_config=bnb_config,\n","#     device_map=device_map\n","# )\n","# base_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
